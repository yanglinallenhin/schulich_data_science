{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature rankings in descending order of influence:\n",
      "     Feature  Importance\n",
      "3     Income    0.210101\n",
      "6      Party    0.165702\n",
      "5   Religion    0.149855\n",
      "4     Region    0.145295\n",
      "1  Education    0.136898\n",
      "2        Age    0.126526\n",
      "0       Race    0.065624\n",
      "Top 3 features: ['Income', 'Party', 'Religion']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = '/Users/allenyang/Downloads/project-clean-data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows with NaN values in any relevant columns\n",
    "data_cleaned = data.dropna(subset=['F_RACECMB', 'F_EDUCCAT2', 'F_AGECAT', 'F_INC_SDT1', 'F_CREGION', 'F_RELIG', 'F_PARTY_FINAL', 'Social Media'])\n",
    "\n",
    "# Selecting relevant columns and renaming them\n",
    "data_cleaned = data_cleaned[['F_RACECMB', 'F_EDUCCAT2', 'F_AGECAT', 'F_INC_SDT1', 'F_CREGION', 'F_RELIG', 'F_PARTY_FINAL', 'Social Media']]\n",
    "data_cleaned.columns = ['Race', 'Education', 'Age', 'Income', 'Region', 'Religion', 'Party', 'Attitude']\n",
    "\n",
    "# Mapping target variable\n",
    "attitude_mapping = {'Bad idea for society': 0, 'Good idea for society': 1, 'Neither good nor bad': 2}\n",
    "data_cleaned['Attitude'] = data_cleaned['Attitude'].map(attitude_mapping)\n",
    "\n",
    "# Ensure no NaN values exist in the cleaned data\n",
    "data_cleaned = data_cleaned.dropna()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in ['Race', 'Education', 'Age', 'Income', 'Region', 'Religion', 'Party']:\n",
    "    le = LabelEncoder()\n",
    "    data_cleaned[column] = le.fit_transform(data_cleaned[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Splitting features and target\n",
    "X = data_cleaned.drop('Attitude', axis=1)\n",
    "y = data_cleaned['Attitude']\n",
    "\n",
    "# Ensure X and y have consistent length\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Use feature importances from the decision tree to rank features\n",
    "importances = clf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print all features ranked by their importance\n",
    "print(\"Feature rankings in descending order of influence:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Select the top 3 features\n",
    "top_3_features = feature_importance_df['Feature'].head(3).tolist()\n",
    "\n",
    "print(\"Top 3 features:\", top_3_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'customers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m transactions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/transactions_final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Merge the datasets on customer_id\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[43mcustomers\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(transactions, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m                        \u001b[38;5;241m.\u001b[39mmerge(engagements, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m                        \u001b[38;5;241m.\u001b[39mmerge(marketing, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Handle missing values\u001b[39;00m\n\u001b[1;32m     21\u001b[0m median_age \u001b[38;5;241m=\u001b[39m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'customers' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load the datasets\n",
    "customers_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/customers_final.csv')\n",
    "engagements_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/engagements_final.csv')\n",
    "marketing_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/marketing_final.csv')\n",
    "transactions_df = pd.read_csv('https://raw.githubusercontent.com/delinai/schulich_ds1_2024/main/Datasets/transactions_final.csv')\n",
    "\n",
    "# Merge the datasets on customer_id\n",
    "merged_data = customers.merge(transactions, on='customer_id', how='left') \\\n",
    "                       .merge(engagements, on='customer_id', how='left') \\\n",
    "                       .merge(marketing, on='customer_id', how='left')\n",
    "\n",
    "# Handle missing values\n",
    "median_age = merged_data['age'].median()\n",
    "merged_data['age'].fillna(median_age, inplace=True)\n",
    "\n",
    "most_frequent_gender = merged_data['gender'].mode()[0]\n",
    "merged_data['gender'].fillna(most_frequent_gender, inplace=True)\n",
    "\n",
    "# Ensure date columns are in datetime format\n",
    "merged_data['join_date'] = pd.to_datetime(merged_data['join_date'])\n",
    "merged_data['last_purchase_date'] = pd.to_datetime(merged_data['last_purchase_date'])\n",
    "\n",
    "# Calculate customer lifespan in months\n",
    "merged_data['customer_lifespan_months'] = (merged_data['last_purchase_date'] - merged_data['join_date']).dt.days / 30\n",
    "\n",
    "# Calculate transactions per month\n",
    "merged_data['transactions_per_month'] = merged_data['total_transactions'] / merged_data['customer_lifespan_months']\n",
    "\n",
    "# Calculate average spending per month\n",
    "merged_data['average_spending_per_month'] = merged_data['avg_transaction_amount'] * merged_data['transactions_per_month']\n",
    "\n",
    "# Calculate the new CLV\n",
    "merged_data['CLV'] = merged_data['average_spending_per_month'] * merged_data['customer_lifespan_months']\n",
    "\n",
    "# Drop rows with missing CLV\n",
    "merged_data = merged_data.dropna(subset=['CLV'])\n",
    "\n",
    "# Select relevant features including gender\n",
    "X = merged_data[['age', 'gender', 'total_transactions', 'avg_transaction_amount', 'recency', 'number_of_site_visits', 'number_of_emails_opened', 'number_of_clicks']]\n",
    "X = pd.get_dummies(X, columns=['gender'], drop_first=True)\n",
    "\n",
    "y = merged_data['CLV']\n",
    "\n",
    "# Split the data into training, testing, and prediction sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_test, X_predict, y_test, y_predict = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Predict on the prediction set\n",
    "y_pred_future = model.predict(X_predict)\n",
    "\n",
    "# Print the results\n",
    "print(f'MAE: {mae}, RMSE: {rmse}, R2: {r2}')\n",
    "print(f'Future Predictions: {y_pred_future[:10]}')  # Display the first 10 predictions for the prediction set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
